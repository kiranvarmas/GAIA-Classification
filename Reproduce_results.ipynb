{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "rn.seed(10)\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(10)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#0 1 2 3 4 5 \n",
    "#2 3 4 5 6 8\n",
    "\n",
    "#0 1 2 3\n",
    "#2 3 4 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_recall_results = []\n",
    "flat_precision_results=[]\n",
    "flat_accuracy_results=[]\n",
    "flat_balanced_accuracy=[]\n",
    "flat_f1_val=[]\n",
    "\n",
    "stack_recall_results = []\n",
    "stack_precision_results=[]\n",
    "stack_accuracy_results=[]\n",
    "stack_balanced_accuracy=[]\n",
    "stack_f1_val=[]\n",
    "\n",
    "base_recall_results = []\n",
    "base_precision_results=[]\n",
    "base_accuracy_results=[]\n",
    "base_balanced_accuracy=[]\n",
    "base_f1_val=[]\n",
    "\n",
    "sub_recall_results = []\n",
    "sub_precision_results=[]\n",
    "sub_accuracy_results=[]\n",
    "sub_balanced_accuracy=[]\n",
    "sub_f1_val=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([3, 4, 5, 6], dtype=int64), array([ 226, 3166, 3583,  874], dtype=int64))\n",
      "(array([3, 4, 9], dtype=int64), array([ 226, 3166, 4457], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "filtered_features_data = pd.read_csv(\"C:/kv/Thesus/Docs/filtered_features_data.csv\",header=0)\n",
    "filtered_features_data = filtered_features_data.loc[~filtered_features_data.sub_label.isin([2,8])]\n",
    "\n",
    "print(np.unique(filtered_features_data.sub_label,return_counts=True))\n",
    "print(np.unique(filtered_features_data.main_label,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_features_data[filtered_features_data.columns.difference(['source_id','sub_label','main_label'])]\n",
    "#y_main = filtered_features_data.main_label\n",
    "#y_sub = filtered_features_data.sub_label\n",
    "y = filtered_features_data[['sub_label','main_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del filtered_features_data\n",
    "X=X.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dense,Dropout\n",
    "from keras.models import Model,Sequential\n",
    "from keras.initializers import glorot_normal,glorot_uniform\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def flat_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=42, activation='relu',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    optimizer=Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "def stack_model():\n",
    "    #comes from flat\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=46, activation='relu',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    optimizer=Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "def base_class_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=42, activation='relu',kernel_initializer=glorot_normal(seed=100)))\n",
    "    \n",
    "    model.add(Dense(30,activation='relu'))\n",
    "    \n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    optimizer=Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def sub_class_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=45, activation='relu',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    optimizer=Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def deep_flat_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=42, activation='relu',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(45,activation='relu'))\n",
    "    model.add(Dense(40,activation='relu'))\n",
    "    model.add(Dense(35,activation='relu'))\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "    model.add(Dense(25,activation='relu'))\n",
    "    model.add(Dense(15,activation='relu'))\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    optimizer=Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def deep_stack_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=46, activation='relu',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(45,activation='relu'))\n",
    "    model.add(Dense(40,activation='relu'))\n",
    "    model.add(Dense(35,activation='relu'))\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "    model.add(Dense(25,activation='relu'))\n",
    "    model.add(Dense(15,activation='relu'))\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    optimizer=Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def deep_base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=42, activation='relu',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(45,activation='relu'))\n",
    "    model.add(Dense(40,activation='relu'))\n",
    "    model.add(Dense(35,activation='relu'))\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "    model.add(Dense(25,activation='relu'))\n",
    "    model.add(Dense(15,activation='relu'))\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    optimizer=Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "def deep_sub_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=45, activation='relu',kernel_initializer=glorot_normal(seed=100)))\n",
    "    model.add(Dense(45,activation='relu'))\n",
    "    model.add(Dense(40,activation='relu'))\n",
    "    model.add(Dense(35,activation='relu'))\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "    model.add(Dense(25,activation='relu'))\n",
    "    model.add(Dense(15,activation='relu'))\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    optimizer=Adam(lr=0.001)\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['sparse_categorical_accuracy'])#sparse_categorical_accuracy\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getTrainTestData(X,y,random_seed):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=random_seed)\n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "\n",
    "def performScaling(X_train,X_test,y_train,y_test,col_list):\n",
    "\n",
    "    from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "    scaler = RobustScaler()\n",
    "    #scaler = MinMaxScaler()\n",
    "    X_train_scl = scaler.fit_transform(X_train) #X_train #X_train_res\n",
    "    X_test_scl = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scl = pd.DataFrame(X_train_scl, columns = col_list) #X-train_scl\n",
    "    X_test_scl  = pd.DataFrame(X_test_scl, columns = col_list)\n",
    "\n",
    "    y_train=y_train.reset_index(drop=True)\n",
    "    y_test =y_test.reset_index(drop=True)\n",
    "\n",
    "    scaled_train_df = pd.concat([X_train_scl,y_train],axis=1)\n",
    "    scaled_test_df = pd.concat([X_test_scl,y_test],axis=1)\n",
    "    \n",
    "    #X_train_scl,X_test_scl,y_train,y_test\n",
    "    return scaled_train_df,scaled_test_df\n",
    "\n",
    "   \n",
    "\n",
    "def getExperimentData(train_df,test_df,label_type):\n",
    "    \n",
    "    if (label_type==\"base\"):\n",
    "        X_train = train_df[train_df.columns.difference(['sub_label','main_label'])]\n",
    "        Y_train = train_df[['main_label']]\n",
    "\n",
    "        X_test = test_df[test_df.columns.difference(['sub_label','main_label'])]\n",
    "        Y_test = test_df[['main_label']]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        X_train = train_df[train_df.columns.difference(['sub_label','main_label'])]\n",
    "        Y_train = train_df[['sub_label']]\n",
    "\n",
    "        X_test = test_df[test_df.columns.difference(['sub_label','main_label'])]\n",
    "        Y_test = test_df[['sub_label']]\n",
    "    \n",
    "    print(\"X_train:\", X_train.shape)\n",
    "    print(\"Y_train:\", Y_train.shape)\n",
    "\n",
    "    print(\"X_test\", X_test.shape)\n",
    "    print(\"Y_test\", Y_test.shape)\n",
    "\n",
    "    print(\"Y_train:\", np.unique(Y_train,return_counts=True))\n",
    "    print(\"Y_test:\", np.unique( Y_test,return_counts=True))\n",
    "    \n",
    "    return X_train,X_test,Y_train,Y_test\n",
    "\n",
    "\n",
    "def performSMOTE(X_train,X_test,y_train,y_test,col_list):\n",
    "   \n",
    "\n",
    "    print(\"Before OverSampling, counts of label '2': {}\".format(sum(y_train.sub_label==2))) #sum(hy_train==0)\n",
    "    print(\"Before OverSampling, counts of label '3': {}\".format(sum(y_train.sub_label==3)))\n",
    "\n",
    "    print(\"Before OverSampling, counts of label '4': {} \\n\".format(sum(y_train.sub_label==4)))\n",
    "    print(\"Before OverSampling, counts of label '5': {} \\n\".format(sum(y_train.sub_label==5)))\n",
    "    print(\"Before OverSampling, counts of label '6': {} \\n\".format(sum(y_train.sub_label==6)))\n",
    "    print(\"Before OverSampling, counts of label '8': {} \\n\".format(sum(y_train.sub_label==8)))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Applying SMOTE\")\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN\n",
    "    sm = SMOTE(random_state=2)\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train.sub_label.ravel())\n",
    "\n",
    "    #ad = ADASYN(sampling_strategy='auto',random_state=2)\n",
    "    #X_train_res, y_train_res = ad.fit_resample(X_train, y_train.sub_label.ravel())\n",
    "\n",
    "    print(\"After OverSampling, counts of label '2': {}\".format(sum(y_train_res==2))) #sum(hy_train==0)\n",
    "    \n",
    "    print(\"After OverSampling, counts of label '3': {}\".format(sum(y_train_res==3)))\n",
    "\n",
    "    print(\"After OverSampling, counts of label '4': {} \\n\".format(sum(y_train_res==4)))\n",
    "    print(\"After OverSampling, counts of label '5': {} \\n\".format(sum(y_train_res==5)))\n",
    "    print(\"After OverSampling, counts of label '6': {} \\n\".format(sum(y_train_res==6)))\n",
    "    print(\"After OverSampling, counts of label '8': {}\".format(sum(y_train_res==8)))\n",
    "\n",
    "\n",
    "    print(\" train\", np.unique(y_train_res,return_counts=True))\n",
    "    \n",
    "    X_train_res = pd.DataFrame(X_train_res, columns=col_list)\n",
    "    y_train_res = pd.DataFrame(y_train_res, columns=['sub_label'])\n",
    "\n",
    "    y_train_res['main_label']=y_train_res.sub_label\n",
    "    \n",
    "    \n",
    "    #y_train_res.loc[y_train_res.sub_label==8,['main_label']]=2\n",
    "    y_train_res.loc[y_train_res.sub_label==5,['main_label']]=9\n",
    "    y_train_res.loc[y_train_res.sub_label==6,['main_label']]=9\n",
    "    \n",
    "    \n",
    "    ##y_train_res=y_train_res.reset_index(drop=True)\n",
    "    #y_test =y_test.reset_index(drop=True)\n",
    "\n",
    "    smote_train_df = pd.concat([X_train_res,y_train_res],axis=1)\n",
    "    smote_test_df = pd.concat([X_test,y_test],axis=1)\n",
    "    \n",
    "    #print(\" test\", np.unique(y_test,return_counts=True))\n",
    "\n",
    "    return smote_train_df,smote_test_df\n",
    "\n",
    "def get_weights(y_train):\n",
    "    \n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                     np.unique(y_train.main_label),\n",
    "                                                     y_train.main_label)\n",
    "\n",
    "\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    return class_weight_dict\n",
    "      \n",
    "\n",
    "def get_estimator(model,X,y,weights):\n",
    "    estimator = train_model(model,X,y)\n",
    "    estimator.fit(X,y,class_weight=weights)\n",
    " \n",
    "    return estimator\n",
    "\n",
    "def train_model(base,X,y):\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold,cross_val_score  \n",
    "    estimator = KerasClassifier(build_fn=base, epochs=300, batch_size=100,verbose=0)\n",
    "   \n",
    "        \n",
    "    #kfold = StratifiedKFold(n_splits=10, shuffle=True,random_state=100)#random_state=0\n",
    "    #results = cross_val_score(estimator,X,y, cv=kfold,scoring='balanced_accuracy')                              \n",
    "    #print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))  \n",
    "    print(\"skipped cv\")\n",
    "    return estimator\n",
    "\n",
    "def evaluate_model(classifier,X,y):\n",
    "   \n",
    "    \n",
    "    predictions,predict_prob,accuracy,bal_accuracy,precision,recall,f1,confusion_mat = get_metrics(classifier,X,y)\n",
    "    \n",
    "    print_metrics(accuracy,bal_accuracy,precision,recall,f1,confusion_mat)\n",
    "    \n",
    "    return predictions,predict_prob,accuracy,bal_accuracy,precision,recall,f1,confusion_mat\n",
    "\n",
    "def get_metrics(classifier,X,y):\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,balanced_accuracy_score,f1_score\n",
    "    predictions,predict_prob = get_predictions(classifier,X)\n",
    "    accuracy = accuracy_score(y,predictions)\n",
    "    balanced_accuracy= balanced_accuracy_score(y,predictions)\n",
    "    precision=precision_score(y,predictions,average=None)\n",
    "    recall = recall_score(y,predictions,average=None)\n",
    "    confusion_mat = confusion_matrix(y,predictions)\n",
    "    f1 = f1_score(y, predictions, labels=None, pos_label=1,average=None, sample_weight=None)\n",
    "    \n",
    "    return predictions,predict_prob,accuracy,balanced_accuracy,precision,recall,f1,confusion_mat\n",
    "\n",
    "def get_predictions(classifier,X):\n",
    "    return classifier.predict(X), classifier.predict_proba(X)\n",
    "\n",
    "\n",
    "def print_metrics(accuracy,balanced_accuracy,precision,recall,f1,confusion_mat):    \n",
    "    print(\" Accuracy\", accuracy)\n",
    "    print(\" Balanced accuracy\",balanced_accuracy)\n",
    "    print(\" Recall\",recall )\n",
    "    print(\" Precision\",precision)\n",
    "    print(\"f1\",f1)\n",
    "    print(confusion_mat)\n",
    "    \n",
    "\n",
    "def get_probs(dataset,predict_prob,method):\n",
    "    if method==\"stack\":\n",
    "        ip = pd.DataFrame(predict_prob, columns=['prob1','prob2','prob3','prob4'])\n",
    "        X = dataset.copy()\n",
    "        X['prob1']=ip[['prob1']]\n",
    "        X['prob2']=ip[['prob2']]\n",
    "        X['prob3']=ip[['prob3']]\n",
    "        X['prob4']=ip[['prob4']]\n",
    "        #X['prob5']=ip[['prob5']]\n",
    "        #X['prob6']=ip[['prob6']]\n",
    "        \n",
    "    else:\n",
    "        ip = pd.DataFrame(predict_prob, columns=['prob1','prob2','prob3'])\n",
    "        X = dataset.copy()\n",
    "        X['prob1']=ip[['prob1']]\n",
    "        X['prob2']=ip[['prob2']]\n",
    "        X['prob3']=ip[['prob3']]\n",
    "        #X['prob4']=ip[['prob4']]\n",
    "\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def put_in_df(metric,hierarchy):\n",
    "    \n",
    "    if(hierarchy==\"accuracy\"):\n",
    "        headers=['accuracy','bal_accuracy']\n",
    "    \n",
    "    elif (hierarchy==\"base\"):\n",
    "        headers = ['class3','class4','class9']\n",
    "        \n",
    "    else:\n",
    "        headers= ['class3','class4','class5','class6']\n",
    "    \n",
    "    df = pd.DataFrame(metric,columns=headers)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def show_class_plots(X,y):\n",
    "    classes = list(np.unique(y))\n",
    "   \n",
    "    from matplotlib import pyplot\n",
    "    from numpy import where\n",
    "    for class_value in classes:\n",
    "        # select indices of points with the class label\n",
    "\n",
    "        X_arr = np.asarray(X)\n",
    "\n",
    "\n",
    "        row_ix = where(y == class_value)\n",
    "\n",
    "        # scatter plot for points with a different color\n",
    "        pyplot.scatter(X_arr[row_ix, 0], X_arr[row_ix, 1])\n",
    "    # show plot\n",
    "    pyplot.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_class_plots(X,y.sub_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (5886, 42)\n",
      "Y_train: (5886, 1)\n",
      "X_test (1963, 42)\n",
      "Y_test (1963, 1)\n",
      "Y_train: (array([3, 4, 5, 6], dtype=int64), array([ 174, 2370, 2698,  644], dtype=int64))\n",
      "Y_test: (array([3, 4, 5, 6], dtype=int64), array([ 52, 796, 885, 230], dtype=int64))\n",
      "X_train: (5886, 42)\n",
      "Y_train: (5886, 1)\n",
      "X_test (1963, 42)\n",
      "Y_test (1963, 1)\n",
      "Y_train: (array([3, 4, 9], dtype=int64), array([ 174, 2370, 3342], dtype=int64))\n",
      "Y_test: (array([3, 4, 9], dtype=int64), array([  52,  796, 1115], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "feature_list=X.columns\n",
    "X_train, X_test, y_train, y_test = getTrainTestData(X,y,100)\n",
    "\n",
    "\n",
    "\n",
    "scaled_train_df, scaled_test_df = performScaling(X_train,X_test,y_train,y_test,feature_list)\n",
    "#X_train_scl,X_test_scl,y_train_scl,y_test_scl = performScaling(X_train,X_test,y_train,y_test,feature_list)\n",
    "#smote_train_df,smote_test_df = performSMOTE(X_train_scl,X_test_scl,y_train_scl,y_test_scl,feature_list)\n",
    "flat_X_train,flat_X_test,flat_Y_train,flat_Y_test = getExperimentData(scaled_train_df, scaled_test_df,\"flat\")\n",
    "base_X_train,base_X_test,base_Y_train,base_Y_test = getExperimentData(scaled_train_df, scaled_test_df,\"base\")\n",
    "#sub_X_train,sub_X_test,sub_Y_train,sub_Y_test = getExperimentData(smote_train_df, smote_test_df,\"sub\")\n",
    "\n",
    "#show_class_plots(flat_X_train,flat_Y_train.sub_label)\n",
    "#show_class_plots(flat_X_test,flat_Y_test.sub_label)\n",
    "del X_train,X_test,y_train,y_test\n",
    "#X_train_scl,X_test_scl,y_train_scl,y_test_scl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.7809475292919001\n",
      " Balanced accuracy 0.626752252988769\n",
      " Recall [0.26923077 0.83291457 0.81355932 0.59130435]\n",
      " Precision [0.2295082  0.85218509 0.79034029 0.63849765]\n",
      "f1 [0.24778761 0.84243964 0.80178174 0.61399549]\n",
      "[[ 14   7  23   8]\n",
      " [  4 663 107  22]\n",
      " [ 33  85 720  47]\n",
      " [ 10  23  61 136]]\n"
     ]
    }
   ],
   "source": [
    "weights={}\n",
    "\n",
    "flat_estimator = get_estimator(deep_flat_model,flat_X_train,flat_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "flat_train_predictions,flat_train_predict_prob = get_predictions(flat_estimator,flat_X_train)\n",
    "\n",
    "flat_predictions,flat_predict_prob,flat_accuracy,flat_bal_accuracy,flat_precision,flat_recall,flat_f1,flat_confusion_mat=evaluate_model(flat_estimator,flat_X_test,flat_Y_test)\n",
    "\n",
    "flat_recall_results.append(flat_recall)\n",
    "flat_precision_results.append(flat_precision)\n",
    "flat_accuracy_results.append([flat_accuracy,flat_bal_accuracy])\n",
    "flat_f1_val.append(flat_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7936831380539989, 0.6107089911928621],\n",
       " [0.7901171676006113, 0.5968859587631381],\n",
       " [0.7758532857870606, 0.5969100325068255],\n",
       " [0.7931737137035151, 0.5988537723768925],\n",
       " [0.7865511971472237, 0.6285378609291342],\n",
       " [0.803871625063678, 0.630889019268915],\n",
       " [0.7992868059093224, 0.639376771131138],\n",
       " [0.8053998981151299, 0.6178574157029195],\n",
       " [0.7987773815588385, 0.6337710718286044],\n",
       " [0.7809475292919001, 0.626752252988769]]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "flat_accuracy_results\n",
    "#flat_Y_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_X_train = get_probs(flat_X_train,flat_train_predict_prob,\"stack\")\n",
    "stack_X_test = get_probs(flat_X_test,flat_predict_prob,\"stack\")\n",
    "\n",
    "stack_Y_train= scaled_train_df[['sub_label']]\n",
    "stack_Y_test = scaled_test_df[['sub_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.8028527763627101\n",
      " Balanced accuracy 0.65493648708558\n",
      " Recall [0.32692308 0.86683417 0.8259887  0.6       ]\n",
      " Precision [0.28813559 0.87231353 0.81493868 0.63888889]\n",
      "f1 [0.30630631 0.86956522 0.82042649 0.61883408]\n",
      "[[ 17   6  21   8]\n",
      " [  3 690  82  21]\n",
      " [ 32  73 731  49]\n",
      " [  7  22  63 138]]\n"
     ]
    }
   ],
   "source": [
    "weights={}\n",
    "\n",
    "stack_estimator = get_estimator(deep_stack_model,stack_X_train,stack_Y_train,weights)\n",
    "#stack_estimator = get_estimator(stack_class_model,stack_X_train,stack_Y_train,50,weights)\n",
    "\n",
    "stack_train_predictions,stack_train_predict_prob = get_predictions(stack_estimator,stack_X_train)\n",
    "\n",
    "stack_predictions,stack_predict_prob,stack_accuracy,stack_bal_accuracy,stack_precision,stack_recall,stack_f1,stack_confusion_mat=evaluate_model(stack_estimator,stack_X_test,stack_Y_test)\n",
    "\n",
    "stack_recall_results.append(stack_recall)\n",
    "stack_precision_results.append(stack_precision)\n",
    "stack_accuracy_results.append([stack_accuracy,stack_bal_accuracy])\n",
    "stack_f1_val.append(stack_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7906265919510953, 0.6067122705832383],\n",
       " [0.7885888945491595, 0.598882613637497],\n",
       " [0.7819663779928681, 0.5965179625991252],\n",
       " [0.7962302598064187, 0.6047500959494093],\n",
       " [0.7880794701986755, 0.6322236811227574],\n",
       " [0.8064187468160978, 0.6315552834295575],\n",
       " [0.8023433520122262, 0.633369575681977],\n",
       " [0.8099847172694855, 0.6261408238421003],\n",
       " [0.804381049414162, 0.6296572673295674],\n",
       " [0.8028527763627101, 0.65493648708558]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack_accuracy_results.pop()\n",
    "#stack_precision_results.pop()\n",
    "\n",
    "#stack_recall_results.pop()\n",
    "#stack_f1_val.pop()\n",
    "stack_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.8680590932246561\n",
      " Balanced accuracy 0.6339044018528887\n",
      " Recall [0.13461538 0.85678392 0.9103139 ]\n",
      " Precision [0.23333333 0.88686606 0.87199313]\n",
      "f1 [0.17073171 0.8715655  0.89074155]\n",
      "[[   7    6   39]\n",
      " [   4  682  110]\n",
      " [  19   81 1015]]\n"
     ]
    }
   ],
   "source": [
    "#weights=get_weights(base_Y_train)\n",
    "weights={}\n",
    "base_estimator = get_estimator(deep_base_model,base_X_train,base_Y_train,weights)\n",
    "#base_estimator = get_estimator(base_class_model,base_X_train,base_Y_train,50,weights)\n",
    "\n",
    "base_train_predictions,base_train_predict_prob = get_predictions(base_estimator,base_X_train)\n",
    "\n",
    "base_predictions,base_predict_prob,base_accuracy,base_bal_accuracy,base_precision,base_recall,base_f1,base_confusion_mat=evaluate_model(base_estimator,base_X_test,base_Y_test)\n",
    "\n",
    "base_recall_results.append(base_recall)\n",
    "base_precision_results.append(base_precision)\n",
    "base_accuracy_results.append([base_accuracy,base_bal_accuracy])\n",
    "base_f1_val.append(base_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8385124808965868, 0.6105678756022782],\n",
       " [0.8420784513499745, 0.6149600059750231],\n",
       " [0.8609271523178808, 0.664471865783007],\n",
       " [0.8588894549159449, 0.6487040010778734],\n",
       " [0.8644931227712684, 0.6548776279688263],\n",
       " [0.8604177279673968, 0.631741092021917],\n",
       " [0.8619460010188487, 0.6431456849775502],\n",
       " [0.8680590932246561, 0.6613465813020788],\n",
       " [0.8451349974528782, 0.6335100303660236],\n",
       " [0.8680590932246561, 0.6339044018528887]]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#base_accuracy_results.append([0.7834394904458599, 0.711897046524023])\n",
    "#base_recall_results.pop()\n",
    "#base_precision_results.pop()\n",
    "#base_accuracy_results.pop()\n",
    "#base_f1_val.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5886, 45)\n",
      "(1963, 45)\n",
      "(array([3, 4, 5, 6], dtype=int64), array([ 174, 2370, 2698,  644], dtype=int64))\n",
      "(array([3, 4, 5, 6], dtype=int64), array([ 52, 796, 885, 230], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "sub_X_train = get_probs(base_X_train,base_train_predict_prob,\"base\")\n",
    "sub_X_test = get_probs(base_X_test,base_predict_prob,\"base\")\n",
    "\n",
    "sub_Y_train= scaled_train_df[['sub_label']]\n",
    "sub_Y_test = scaled_test_df[['sub_label']]\n",
    "\n",
    "print(sub_X_train.shape)\n",
    "print(sub_X_test.shape)\n",
    "print(np.unique(sub_Y_train,return_counts=True))\n",
    "print(np.unique(sub_Y_test, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cols = sub_X_train.columns\n",
    "#sub_X_train_scl,sub_X_test_scl,sub_Y_train_scl,sub_Y_test_scl = performScaling(sub_X_train,sub_X_test,sub_Y_train,sub_Y_test,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped cv\n",
      " Accuracy 0.8099847172694855\n",
      " Balanced accuracy 0.6254851933837318\n",
      " Recall [0.13461538 0.8580402  0.8440678  0.66521739]\n",
      " Precision [0.19444444 0.88816645 0.81907895 0.62195122]\n",
      "f1 [0.15909091 0.87284345 0.83138564 0.64285714]\n",
      "[[  7   6  28  11]\n",
      " [  7 683  78  28]\n",
      " [ 17  67 747  54]\n",
      " [  5  13  59 153]]\n"
     ]
    }
   ],
   "source": [
    "weights={}\n",
    "\n",
    "sub_estimator = get_estimator(deep_sub_model,sub_X_train,sub_Y_train,weights)\n",
    "#sub_estimator = get_estimator(sub_class_model,sub_X_train,sub_Y_train,50,weights)\n",
    "\n",
    "sub_train_predictions,sub_train_predict_prob = get_predictions(sub_estimator,sub_X_train)\n",
    "\n",
    "sub_predictions,sub_predict_prob,sub_accuracy,sub_bal_accuracy,sub_precision,sub_recall,sub_f1,sub_confusion_mat=evaluate_model(sub_estimator,sub_X_test,sub_Y_test)\n",
    "\n",
    "sub_recall_results.append(sub_recall)\n",
    "sub_precision_results.append(sub_precision)\n",
    "sub_accuracy_results.append([sub_accuracy,sub_bal_accuracy])\n",
    "sub_f1_val.append(sub_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7789098318899643, 0.5987921804050836],\n",
       " [0.7707590422822211, 0.5867156459919523],\n",
       " [0.7957208354559347, 0.6218542452484356],\n",
       " [0.7890983188996434, 0.6269803694589609],\n",
       " [0.8023433520122262, 0.6332964920759473],\n",
       " [0.8003056546102904, 0.6072328430670535],\n",
       " [0.7931737137035151, 0.5963449992535262],\n",
       " [0.804381049414162, 0.6205673526445326],\n",
       " [0.7911360163015793, 0.6071489129016926],\n",
       " [0.8099847172694855, 0.6254851933837318]]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_recall_results.pop()\n",
    "#sub_precision_results.pop()\n",
    "#sub_accuracy_results.pop()\n",
    "#sub_f1_val.pop()\n",
    "#[[0.8166104553119731, 0.4302010414046331],\n",
    "# [0.8204047217537943, 0.4361940189393212],\n",
    "# [0.8204047217537943, 0.44585561851903416],\n",
    "# [0.8182967959527825, 0.4570565603256556],\n",
    "# [0.8166104553119731, 0.4599477653456745],\n",
    "# [0.8182967959527825, 0.43322756627375325]]\n",
    "#base_precision_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base_precision_df=put_in_df(base_precision_results,\"base\")\n",
    "base_recall_df=put_in_df(base_recall_results,\"base\")\n",
    "base_f1_df=put_in_df(base_f1_val,\"base\")\n",
    "\n",
    "sub_precision_df=put_in_df(sub_precision_results,\"sub\")\n",
    "sub_recall_df=put_in_df(sub_recall_results,\"sub\")\n",
    "sub_f1_df=put_in_df(sub_f1_val,\"sub\")\n",
    "\n",
    "\n",
    "flat_precision_df=put_in_df(flat_precision_results,\"flat\")\n",
    "flat_recall_df=put_in_df(flat_recall_results,\"flat\")\n",
    "flat_f1_df=put_in_df(flat_f1_val,\"flat\")\n",
    "\n",
    "\n",
    "stack_precision_df=put_in_df(stack_precision_results,\"flat\")\n",
    "stack_recall_df=put_in_df(stack_recall_results,\"flat\")\n",
    "stack_f1_df=put_in_df(stack_f1_val,\"flat\")\n",
    "\n",
    "\n",
    "flat_acc_df = put_in_df(flat_accuracy_results,\"accuracy\")\n",
    "stack_acc_df = put_in_df(stack_accuracy_results,\"accuracy\")\n",
    "base_acc_df = put_in_df(base_accuracy_results,\"accuracy\")\n",
    "sub_acc_df = put_in_df(sub_accuracy_results,\"accuracy\")\n",
    "\n",
    "\n",
    "flat_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_flat_precision.csv')\n",
    "flat_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_flat_recall.csv')\n",
    "flat_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_flat_f1.csv')\n",
    "\n",
    "stack_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_stack_precision.csv')\n",
    "stack_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_stack_recall.csv')\n",
    "stack_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_stack_f1.csv')\n",
    "\n",
    "\n",
    "base_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_base_precision.csv')\n",
    "base_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_base_recall.csv')\n",
    "base_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_base_f1.csv')\n",
    "\n",
    "\n",
    "sub_precision_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_sub_precision.csv')\n",
    "sub_recall_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_sub_recall.csv')\n",
    "sub_f1_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_sub_f1.csv')\n",
    "\n",
    "\n",
    "\n",
    "flat_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_flat_acc.csv')\n",
    "stack_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_stack_acc.csv')\n",
    "base_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_base_acc.csv')\n",
    "sub_acc_df.to_csv('C:/kv/Thesus/PLAStiCC/results/gaia/non_smote/deep_sub_acc.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "del stack_precision_df,stack_recall_df,stack_f1_df,flat_precision_df,flat_recall_df,flat_f1_df,base_precision_df,base_recall_df,base_f1_df,sub_precision_df,sub_recall_df,sub_f1_df\n",
    "del flat_acc_df,stack_acc_df,base_acc_df,sub_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sub_precision_df=put_in_df(sub_precision_results,\"sub\")\n",
    "#sub_recall_df=put_in_df(sub_recall_results,\"sub\")\n",
    "#sub_f1_df=put_in_df(sub_f1_val,\"sub\")\n",
    "\n",
    "\n",
    "#base_precision_df=put_in_df(base_precision_results,\"base\")\n",
    "#base_recall_df=put_in_df(base_recall_results,\"base\")\n",
    "#base_f1_df=put_in_df(base_f1_val,\"base\")\n",
    "\n",
    "\n",
    "#flat_precision_df=put_in_df(flat_precision_results,\"flat\")\n",
    "#flat_recall_df=put_in_df(flat_recall_results,\"flat\")\n",
    "#flat_f1_df=put_in_df(flat_f1_val,\"flat\")\n",
    "\n",
    "\n",
    "#stack_precision_df=put_in_df(stack_precision_results,\"flat\")\n",
    "#stack_recall_df=put_in_df(stack_recall_results,\"flat\")\n",
    "#stack_f1_df=put_in_df(stack_f1_val,\"flat\")\n",
    "\n",
    "#flat_precision_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_flat_precision.csv')\n",
    "#flat_recall_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_flat_recall.csv')\n",
    "#flat_f1_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_flat_f1.csv')\n",
    "\n",
    "#base_precision_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_base_precision.csv')\n",
    "#base_recall_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_base_recall.csv')\n",
    "#base_f1_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_base_f1.csv')\n",
    "\n",
    "\n",
    "#sub_precision_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_sub_precision.csv')\n",
    "#sub_recall_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_sub_recall.csv')\n",
    "#sub_f1_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_sub_f1.csv')\n",
    "\n",
    "\n",
    "#stack_precision_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_stack_precision.csv')\n",
    "#stack_recall_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_stack_recall.csv')\n",
    "#stack_f1_df.to_csv('C:/kv/Thesus/final_ops/tan_relu/tr_stack_f1.csv')\n",
    "\n",
    "\n",
    "#del stack_precision_df,stack_recall_df,stack_f1_df,flat_precision_df,flat_recall_df,flat_f1_df,base_precision_df,base_recall_df,base_f1_df,sub_precision_df,sub_recall_df,sub_f1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#[[0.8250421585160203, 0.41364078769907703]]\n",
    "#[[0.877318718381113, 0.5038310200245086]]\n",
    "#[[0.8284148397976391, 0.46965511621865424]]\n",
    "\n",
    "#glorot_normal\n",
    "#[[0.8136593591905565, 0.44069465626973914]]\n",
    "#[[0.8756323777403036, 0.50548378051262]]\n",
    "#[[0.8090219224283305, 0.4557164164824749]]\n",
    "\n",
    "#glorot_uniform\n",
    "#[[0.8237774030354131, 0.47176357587703527]]\n",
    "#[[0.8722596964586846, 0.5088757120591351]]\n",
    "#[[0.8043844856661045, 0.47038757225040745]]\n",
    "\n",
    "#flat\n",
    "#[[0.7995753715498939, 0.6365264017952621],\n",
    "# [0.8220806794055202, 0.6356701349006082],\n",
    "# [0.8169851380042463, 0.6268129529765984],\n",
    "# [0.8292993630573249, 0.6395721852638229],\n",
    "# [0.8199575371549894, 0.6453256436005167],\n",
    "# [0.8229299363057325, 0.6588996873870432]]\n",
    "\n",
    "#base\n",
    "#[[0.8679405520169852, 0.6794615193036666],\n",
    "# [0.8704883227176221, 0.6381314426462021],\n",
    "# [0.8789808917197452, 0.6402200404682626],\n",
    "# [0.8781316348195329, 0.6612129403493042],\n",
    "# [0.8764331210191083, 0.6476376626659439],\n",
    "# [0.8747346072186837, 0.6570557041839692]]\n",
    "\n",
    "#[[0.8029723991507431, 0.6313469499778586],\n",
    "# [0.8059447983014862, 0.6265881369849307],\n",
    "# [0.8110403397027601, 0.6469963909939804],\n",
    "# [0.8110403397027601, 0.6247791388573778],\n",
    "#[0.8182590233545648, 0.626868811614477],\n",
    "# [0.8110403397027601, 0.6382009224940102]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
